{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith 추적 설정\n",
    "\n",
    "- LLM 앱 개발, 모니터링 및 테스트를 위한 플랫폼\n",
    "- https://smith.langchain.com/ 접속하여 회원가입/로그인 진행\n",
    "\n",
    "- 생성한 API Key를 넣고 설정\n",
    "\n",
    "```.env\n",
    "LANGSMITH_TRACING=true\n",
    "LANGSMITH_ENDPOINT=https://api.smith.langchain.com\n",
    "LANGSMITH_API_KEY=API 키\n",
    "LANGSMITH_PROJECT=프로젝트명\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# OPEN AI API 키 설정\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프로젝트: LangChain Test 추적 시작\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def project_setup(name):\n",
    "    os.environ['LANGSMITH_PROJECT'] = name\n",
    "    os.environ['LANGSMITH_TRACING'] = 'true'\n",
    "    print('프로젝트: {} 추적 시작'.format(name))\n",
    "\n",
    "def stop_tracing():\n",
    "    os.environ['LANGSMITH_TRACING'] = 'false'\n",
    "    print('LangSmith 추적 중지')\n",
    "\n",
    "project_setup('LangChain Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open AI가 제공하는 모델\n",
    "\n",
    "- https://platform.openai.com/docs/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODELS = {\n",
    "    \"gpt-4.1\": {\n",
    "        'price': {\n",
    "            'input': 2.00, # per 1M tokens\n",
    "            'cached': 0.5,\n",
    "            'output': 8.00,\n",
    "        }\n",
    "    },\n",
    "    \"gpt-4.1-mini\": {\n",
    "        'price': {\n",
    "            'input': 0.40,\n",
    "            'cached': 0.1,\n",
    "            'output': 1.60,\n",
    "        }\n",
    "    },\n",
    "    \"gpt-4.1-nano\": { # 가장 저렴한 모델\n",
    "        'price': {\n",
    "            'input': 0.10,\n",
    "            'cached': 0.025,\n",
    "            'output': 0.40,\n",
    "        }\n",
    "    },\n",
    "    \"gpt-4o\": {\n",
    "        'price': {\n",
    "            'input': 2.50,\n",
    "            'cached': 1.25,\n",
    "            'output': 10.00,\n",
    "        }\n",
    "    },\n",
    "    \"gpt-4o-mini\": { \n",
    "        'price': {\n",
    "            'input': 0.15,\n",
    "            'cached': 0.075,\n",
    "            'output': 0.60,\n",
    "        }\n",
    "    },\n",
    "    \"o3\": {\n",
    "        'price': {\n",
    "            'input': 10.00,\n",
    "            'cached': 2.50,\n",
    "            'output': 40.00,\n",
    "        }\n",
    "    },\n",
    "    # Usage Tier를 올려야 사용 가능\n",
    "    \"o4-mini\": { # 가장 최신 모델 25-04-16\n",
    "        'price': {\n",
    "            'input': 1.10,\n",
    "            'cached': 0.275,\n",
    "            'output': 4.40,\n",
    "        }\n",
    "    },\n",
    "    # 예전 모델\n",
    "    \"gpt-3.5-turbo\": {\n",
    "        'price': {\n",
    "            'input': 0.5,\n",
    "            'output': 1.50,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "MODEL_NAMES = list(OPENAI_MODELS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-4.1',\n",
       " 'gpt-4.1-mini',\n",
       " 'gpt-4.1-nano',\n",
       " 'gpt-4o',\n",
       " 'gpt-4o-mini',\n",
       " 'o3',\n",
       " 'o4-mini',\n",
       " 'gpt-3.5-turbo']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain 기초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "# 객체 생성\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,  # 창의성 (0.0 ~ 2.0)\n",
    "    max_tokens=2048, # 최대 토큰수\n",
    "    model_name=model_name,  # 모델명\n",
    ") #.bind(logprobs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[답변]: 대한민국의 수도는 서울입니다.\n"
     ]
    }
   ],
   "source": [
    "# 질문\n",
    "question = \"대한민국의 수도는 어디인가요?\"\n",
    "\n",
    "# 답변\n",
    "answer = llm.invoke(question)\n",
    "print(f\"[답변]: {answer.content}\") # 내용만\n",
    "#print(f\"[답변]: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국에는 아름다운 관광지가 많이 있습니다. 아래는 추천하는 10곳과 그 주소입니다.\n",
      "\n",
      "1. **경복궁**\n",
      "   - 주소: 서울특별시 종로구 사직로 161\n",
      "\n",
      "2. **제주도**\n",
      "   - 주소: 제주특별자치도 제주시\n",
      "\n",
      "3. **부산 해운대 해수욕장**\n",
      "   - 주소: 부산광역시 해운대구 해운대해변로 140\n",
      "\n",
      "4. **경주 불국사**\n",
      "   - 주소: 경상북도 경주시 불국로 385\n",
      "\n",
      "5. **남이섬**\n",
      "   - 주소: 강원도 춘천시 남이섬길 1\n",
      "\n",
      "6. **전주 한옥마을**\n",
      "   - 주소: 전라북도 전주시 완산구 기린대로 99\n",
      "\n",
      "7. **설악산 국립공원**\n",
      "   - 주소: 강원도 속초시 설악산로 173\n",
      "\n",
      "8. **안동 하회마을**\n",
      "   - 주소: 경상북도 안동시 풍천면 하회리\n",
      "\n",
      "9. **서울 남산타워 (N서울타워)**\n",
      "   - 주소: 서울특별시 용산구 남산공원길 105\n",
      "\n",
      "10. **광주 무등산**\n",
      "    - 주소: 광주광역시 동구 무등산로 100\n",
      "\n",
      "각 관광지는 고유의 매력을 가지고 있으니, 방문 시 다양한 경험을 즐기시길 바랍니다!"
     ]
    }
   ],
   "source": [
    "# Stream 방식\n",
    "question = \"대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!\"\n",
    "answer = llm.stream(question)\n",
    "\n",
    "for token in answer:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프롬프트\n",
    "\n",
    "- 생성형 모델의 출력을 원하는 방향으로 유도하기 위한 입력 텍스트\n",
    "\n",
    "\n",
    "> #### Role\n",
    "> System Prompt : 사용자의 prompt 이전에 **성능 개선 용도**의 프롬프트로 *페르소나 부여*나 응답 어조 등 **중요 규칙**을 설정 (system)  \n",
    "> User Prompt : 사용자의 입력  (human)  \n",
    "> Assitant: AI 모델을 의미함 (ai)  \n",
    "\n",
    "#### ChatPromptTemplate\n",
    "\n",
    "- 대화목록을 프롬프트로 주입하고자 할 때 활용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages( # 별도의 템플릿으로 관리가능\n",
    "    [\n",
    "        # role, message\n",
    "        (\"system\", \"당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 {name} 입니다.\"),\n",
    "        (\"human\", \"반가워요!\"),\n",
    "        (\"ai\", \"안녕하세요! 무엇을 도와드릴까요?\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 제이슨 입니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='반가워요!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='안녕하세요! 무엇을 도와드릴까요?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='당신의 이름은 무엇입니까?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이런 형태는 자주 쓰진 않음\n",
    "messages = template.format_messages(\n",
    "    name=\"제이슨\", user_input=\"당신의 이름은 무엇입니까?\"\n",
    ")\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'제 이름은 제이슨입니다. 당신은 어떻게 부르길 원하시나요?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주로 이런 형식으로 사용\n",
    "llm.invoke(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "\n",
    "- 여러 작업을 순차적으로 연결해서(Chain) 복잡한 워크플로우를 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'제 이름은 Jason입니다. 당신은 어떻게 부르길 원하시나요?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = template | llm\n",
    "\n",
    "chain.invoke({\n",
    "    \"name\": \"Jason\", \n",
    "    \"user_input\": \"당신의 이름은 무엇입니까?\"\n",
    "}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LECL 인터페이스\n",
    "\n",
    "- `Runnable` 클래스 (사용자 정의 체인을 쉽게 만들 수 있게 하는 프로토콜)\n",
    "\n",
    "> `stream`: 응답의 청크를 스트리밍 (실시간 출력)  \n",
    "> `invoke`: 입력에 대해 체인을 호출 (호출)  \n",
    "> `batch`: 입력 목록에 대해 체인을 호출 (배치단위 호출)\n",
    "\n",
    "- 비동기 방식도 존재\n",
    "> `astream`, `ainvoke`, `abatch`, `astream_log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 주어진 토픽에 대한 요약을 요청하는 프롬프트 템플릿을 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\"{topic} 에 대하여 3문장으로 설명해줘.\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ChatGPT는 OpenAI에서 개발한 대화형 인공지능 모델로, 다양한 주제에 대한 대화 및 질문에 응답할 수 있습니다. 이 모델은 기존에 존재하는 데이터를 학습하여 텍스트 생성 및 이해 능력을 향상시켰습니다. ChatGPT를 활용하면 대화형 서비스나 개인 비서 등 다양한 분야에서 활용할 수 있습니다.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke({\n",
    "    'topic': 'ChatGPT'\n",
    "})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ChatGPT는 대화형 인공지능 프로그램으로, 사용자와 대화를 나누며 다양한 주제에 대해 대화할 수 있습니다. ChatGPT는 자연어 처리 기술을 기반으로 작동하여, 사용자의 질문에 응답하거나 소셜 봇으로 작동할 수 있습니다. ChatGPT는 온라인 상에서 커뮤니케이션 및 정보 교환을 하는 데 도움을 줄 수 있는 유용한 도구입니다.',\n",
       " '인스타그램은 사진과 동영상을 공유하는 소셜 미디어 플랫폼으로, 사용자들은 다양한 필터와 효과를 통해 자신의 콘텐츠를 꾸밀 수 있다. 또한 해시태그를 활용하여 관심사나 주제에 맞는 게시물을 찾아볼 수 있고, 팔로워와 소통하며 커뮤니티를 형성할 수 있다. 인플루언서나 브랜드들 또한 인스타그램을 활용하여 마케팅과 홍보에 활발하게 이용하고 있다.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = chain.batch([\n",
    "    {'topic': 'ChatGPT'},\n",
    "    {'topic': 'Instagram'},\n",
    "])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChatGPT는 혁신적인 자연어 처리 기술을 사용하여 대화 상대와 자연스럽게 대화할 수 있는 AI 챗봇 플랫폼이다. 이 기술은 사용자의 질문에 대답하고 다양한 주제에 대해 대화할 수 있는 지능적이고 학습능력이 있는 모델을 제공한다. ChatGPT를 통해 사람과의 상호작용을 최적화하고 다양한 상황에서 유용한 정보나 서비스를 제공할 수 있다.',\n",
       " 'Instagram은 사진과 동영상을 공유하고 소셜 네트워크 서비스를 이용할 수 있는 모바일 앱이다. 사용자들은 팔로워와 소통하고 다양한 콘텐츠를 즐기며 일상을 공유할 수 있다. 인기 있는 인플루언서나 브랜드들도 활발히 활동하는 플랫폼이다.',\n",
       " '멀티모달은 여러 가지 다른 형식의 정보를 동시에 사용하여 효율적으로 커뮤니케이션하는 방법이다. 이는 텍스트, 이미지, 음성, 비디오 등 다양한 매체를 결합하여 정보를 전달하고 이해하는 데 도움을 준다. 멀티모달을 활용하면 더 다양한 사용자들에게 접근할 수 있고, 효과적인 소통과 학습을 도모할 수 있다.',\n",
       " '프로그래밍은 컴퓨터에게 원하는 작업을 수행하도록 지시하는 과정을 말합니다. 이를 위해 프로그래머는 특정 프로그래밍 언어를 사용하여 코드를 작성하고 디버깅하는 작업을 합니다. 프로그래밍을 통해 다양한 소프트웨어나 애플리케이션을 개발할 수 있으며, 현대 사회에서는 필수적인 기술로 인정받고 있습니다.',\n",
       " '머신러닝은 컴퓨터 시스템이 데이터를 사용해 패턴을 학습하고 예측하는 기술입니다. 이를 통해 프로그램이 스스로 데이터로부터 지식을 습득하고 문제를 해결할 수 있게 됩니다. 머신러닝은 효율적이고 정확한 결정을 내릴 수 있는 스마트한 시스템을 만들기 위해 활발히 연구되고 활용되고 있습니다.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\n",
    "    {'topic': 'ChatGPT'},\n",
    "    {'topic': 'Instagram'},\n",
    "    {'topic': '멀티모달'},\n",
    "    {'topic': '프로그래밍'},\n",
    "    {'topic': '머신러닝'},\n",
    "], config={'max_concurrency': 3}) # 동시 처리 작업 수 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableParallel\n",
    "\n",
    "- 여러 체인을 병렬 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# {country} 의 수도를 물어보는 체인을 생성합니다.\n",
    "chain1 = (\n",
    "    PromptTemplate.from_template(\"{country} 의 수도는 어디야?\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# {country} 의 면적을 물어보는 체인을 생성합니다.\n",
    "chain2 = (\n",
    "    PromptTemplate.from_template(\"{country} 의 면적은 얼마야?\") # 변수가 같을 필요는 없음\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 위의 2개 체인을 동시에 생성하는 병렬 실행 체인을 생성합니다.\n",
    "combined = RunnableParallel(capital=chain1, area=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital': '대한민국의 수도는 서울입니다.', 'area': '대한민국의 총 면적은 약 100,363km² 입니다.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.invoke({\"country\": \"대한민국\"})\n",
    "#combined.invoke({\"country\": \"대한민국\", \"country2\": \"미국\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers (출력 파서)\n",
    "\n",
    "- 출력을 원하는 구조화된 형태로 변환\n",
    "\n",
    "#### PydanticOutputParser\n",
    "- `pydantic` 모듈의 `BaseModel`과 `Field`를 활용하여 객체의 구조를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_conversation = \"\"\"From: 김철수 (chulsoo.kim@bikecorporation.me)\n",
    "To: 이은채 (eunchae@teddyinternational.me)\n",
    "Subject: \"ZENESIS\" 자전거 유통 협력 및 미팅 일정 제안\n",
    "\n",
    "안녕하세요, 이은채 대리님,\n",
    "\n",
    "저는 바이크코퍼레이션의 김철수 상무입니다. 최근 보도자료를 통해 귀사의 신규 자전거 \"ZENESIS\"에 대해 알게 되었습니다. 바이크코퍼레이션은 자전거 제조 및 유통 분야에서 혁신과 품질을 선도하는 기업으로, 이 분야에서의 장기적인 경험과 전문성을 가지고 있습니다.\n",
    "\n",
    "ZENESIS 모델에 대한 상세한 브로슈어를 요청드립니다. 특히 기술 사양, 배터리 성능, 그리고 디자인 측면에 대한 정보가 필요합니다. 이를 통해 저희가 제안할 유통 전략과 마케팅 계획을 보다 구체화할 수 있을 것입니다.\n",
    "\n",
    "또한, 협력 가능성을 더 깊이 논의하기 위해 다음 주 화요일(1월 15일) 오전 10시에 미팅을 제안합니다. 귀사 사무실에서 만나 이야기를 나눌 수 있을까요?\n",
    "\n",
    "감사합니다.\n",
    "\n",
    "김철수\n",
    "상무이사\n",
    "바이크코퍼레이션\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailSummary(BaseModel):\n",
    "    person: str = Field(description=\"메일을 보낸 사람\")\n",
    "    email: str = Field(description=\"메일을 보낸 사람의 이메일 주소\")\n",
    "    subject: str = Field(description=\"메일 제목\")\n",
    "    summary: str = Field(description=\"메일 본문을 요약한 텍스트\")\n",
    "    date: str = Field(description=\"메일 본문에 언급된 미팅 날짜와 시간\")\n",
    "\n",
    "# 출력 형태를 프롬프트에 추가한다\n",
    "\"\"\"  \n",
    "다음의 출력형식으로 답변해줘\n",
    "person: str (메일을 보낸 사람)\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# PydanticOutputParser 생성\n",
    "parser = PydanticOutputParser(pydantic_object=EmailSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"person\": {\"description\": \"메일을 보낸 사람\", \"title\": \"Person\", \"type\": \"string\"}, \"email\": {\"description\": \"메일을 보낸 사람의 이메일 주소\", \"title\": \"Email\", \"type\": \"string\"}, \"subject\": {\"description\": \"메일 제목\", \"title\": \"Subject\", \"type\": \"string\"}, \"summary\": {\"description\": \"메일 본문을 요약한 텍스트\", \"title\": \"Summary\", \"type\": \"string\"}, \"date\": {\"description\": \"메일 본문에 언급된 미팅 날짜와 시간\", \"title\": \"Date\", \"type\": \"string\"}}, \"required\": [\"person\", \"email\", \"subject\", \"summary\", \"date\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a helpful assistant. Please answer the following questions in KOREAN.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "EMAIL CONVERSATION:\n",
    "{email_conversation}\n",
    "\n",
    "FORMAT:\n",
    "{format}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# format 에 PydanticOutputParser의 부분 포맷팅(partial) 추가\n",
    "prompt = prompt.partial(format=parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 을 생성합니다.\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"person\": \"김철수\",\n",
      "    \"email\": \"chulsoo.kim@bikecorporation.me\",\n",
      "    \"subject\": \"\\\"ZENESIS\\\" 자전거 유통 협력 및 미팅 일정 제안\",\n",
      "    \"summary\": \"김철수 상무는 바이크코퍼레이션의 자전거 유통 협력을 위해 ZENESIS 모델의 브로슈어를 요청하고, 기술 사양, 배터리 성능, 디자인 정보를 필요로 합니다. 또한, 협력 논의를 위해 1월 15일 화요일 오전 10시에 미팅을 제안합니다.\",\n",
      "    \"date\": \"1월 15일 화요일 오전 10시\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# chain 을 실행하고 결과를 출력합니다.\n",
    "output = chain.invoke(\n",
    "    {\n",
    "        \"email_conversation\": email_conversation,\n",
    "        \"question\": \"이메일 내용중 주요 내용을 추출해 주세요.\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 결과는 JSON 형태로 출력됩니다.\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Generation\ntext\n  Input should be a valid string [type=string_type, input_value=AIMessage(content='```jso...o': 0, 'reasoning': 0}}), input_type=AIMessage]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m structured_output = \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(structured_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:77\u001b[39m, in \u001b[36mPydanticOutputParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> TBaseModel:\n\u001b[32m     69\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the output of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[32m     70\u001b[39m \n\u001b[32m     71\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m \u001b[33;03m        The parsed pydantic object.\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_core\\output_parsers\\json.py:96\u001b[39m, in \u001b[36mJsonOutputParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m     88\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the output of an LLM call to a JSON object.\u001b[39;00m\n\u001b[32m     89\u001b[39m \n\u001b[32m     90\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m \u001b[33;03m        The parsed JSON object.\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parse_result([\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\anaconda3\\envs\\llm\\Lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\anaconda3\\envs\\llm\\Lib\\site-packages\\pydantic\\main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for Generation\ntext\n  Input should be a valid string [type=string_type, input_value=AIMessage(content='```jso...o': 0, 'reasoning': 0}}), input_type=AIMessage]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_type"
     ]
    }
   ],
   "source": [
    "structured_output = parser.parse(output)\n",
    "print(structured_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 파서를 추가하여 전체 체인을 재구성합니다.(이런 형태임)\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmailSummary(person='김철수', email='chulsoo.kim@bikecorporation.me', subject='\"ZENESIS\" 자전거 유통 협력 및 미팅 일정 제안', summary=\"김철수 상무는 바이크코퍼레이션의 자전거 제조 및 유통 경험을 바탕으로, 테디인터내셔널의 신규 자전거 'ZENESIS'에 대한 유통 협력을 제안하며, 기술 사양, 배터리 성능, 디자인 정보가 포함된 브로슈어를 요청하고, 1월 15일 화요일 오전 10시에 미팅을 제안합니다.\", date='1월 15일 화요일 오전 10시')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain 을 실행하고 결과를 출력합니다.\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"email_conversation\": email_conversation,\n",
    "        \"question\": \"이메일 내용중 주요 내용을 추출해 주세요.\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 결과는 EmailSummary 객체 형태로 출력됩니다.\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김철수\n",
      "chulsoo.kim@bikecorporation.me\n"
     ]
    }
   ],
   "source": [
    "print(response.person)\n",
    "print(response.email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JsonOutputParser\n",
    "\n",
    "- Json 스키마를 지정하여 결과를 JSON 포맷으로 결과를 전달합니다.\n",
    "- 모델의 파라미터 수, 즉 모델이 충분히 커야 정확하게 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 객체를 생성합니다.\n",
    "model = ChatOpenAI(temperature=0, model_name=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 데이터 구조를 정의합니다.\n",
    "class Topic(BaseModel):\n",
    "    description: str = Field(description=\"주제에 대한 간결한 설명\")\n",
    "    hashtags: str = Field(description=\"해시태그 형식의 키워드(2개 이상)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': '지구 온난화는 지구 평균 기온이 상승하여 극심한 기후 변화, 해수면 상승, 생태계 파괴 등 심각한 환경 문제를 초래합니다.',\n",
       " 'hashtags': '#지구온난화 #기후변화 #환경보호 #지구살리기'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질의 작성\n",
    "question = \"지구 온난화의 심각성 대해 알려주세요.\"\n",
    "\n",
    "# 파서를 설정하고 프롬프트 템플릿에 지시사항을 주입합니다.\n",
    "parser = JsonOutputParser(pydantic_object=Topic)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 친절한 AI 어시스턴트 입니다. 질문에 간결하게 답변하세요.\"),\n",
    "        (\"user\", \"#Format: {format_instructions}\\n\\n#Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | model | parser  # 체인을 구성합니다.\n",
    "\n",
    "chain.invoke({\"question\": question})  # 체인을 호출하여 쿼리 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#지구온난화 #기후변화 #환경보호 #지구살리기'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자열이 아니라 \n",
    "result = chain.invoke({\"question\": question})\n",
    "result['hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"description\": {\"description\": \"주제에 대한 간결한 설명\", \"title\": \"Description\", \"type\": \"string\"}, \"hashtags\": {\"description\": \"해시태그 형식의 키워드(2개 이상)\", \"title\": \"Hashtags\", \"type\": \"string\"}}, \"required\": [\"description\", \"hashtags\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic 없이 사용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': '지구 온난화는 대기 중 온실가스 농도의 증가로 인해 지구 평균 기온이 상승하는 현상입니다. 이는 극지방의 빙하 감소, 해수면 상승, 기상이변 증가 등 다양한 환경 변화를 초래합니다.', 'hashtags': ['#지구온난화', '#기후변화', '#온실가스', '#환경보호', '#지구환경']}\n"
     ]
    }
   ],
   "source": [
    "# 질의 작성\n",
    "question = \"\"\" \\\n",
    "\"지구 온난화에 대해 알려주세요. \" \\\n",
    "\"온난화에 대한 설명은 `description`에, 관련 키워드는 `hashtags`에 담아주세요.\" \\\n",
    "\"\"\"\n",
    "\n",
    "# JSON 출력 파서 초기화\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# 프롬프트 템플릿을 설정합니다.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 친절한 AI 어시스턴트 입니다. 질문에 간결하게 답변하세요.\"),\n",
    "        (\"user\", \"#Format: {format_instructions}\\n\\n#Question: {question}\"),\n",
    "        # Format: {format_instructions}\\n\\n\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 지시사항을 프롬프트에 주입합니다.\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# 프롬프트, 모델, 파서를 연결하는 체인 생성\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 체인을 호출하여 쿼리 실행\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "# 출력을 확인합니다.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기타 출력파서\n",
    "\n",
    "> CommaSeparatedListOutputParser  \n",
    "> StructuredOuputParser  \n",
    "> PandasDataFrameOutputParser  \n",
    "> DatetimeOutputParser  \n",
    "> EnumOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENUM (상수값들 모임)\n",
    "\n",
    "class Category:\n",
    "    FLOWER: str\n",
    "    FOOD: str\n",
    "\n",
    "# pd.DataFrame({\n",
    "#     'key': [values]\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 캐싱 (Caching)\n",
    "\n",
    "- `동일한 작업`을 여러 번 수행해야 할 경우 API 호출 횟수를 줄이고 응답 속도를 빠르게 할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InMemoryCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 모델을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\"{country} 에 대해서 200자 내외로 요약해줘\")\n",
    "\n",
    "# 체인을 생성합니다.\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국은 동아시아에 위치한 나라로 경제적으로 선진화된 나라이면서도 전통과 현대가 공존하는 다양한 문화를 지니고 있다. 서울을 중심으로 현대적인 도시들이 발달하고 있으며 한류 열풍으로 한국 문화가 전 세계에 널리 알려져 있다. 또한 한반도 분단으로 인해 북한과의 관계가 여전히 긴장 상태에 있는 것이 한국의 중요한 이슈 중 하나이다. 미래 지향적인 기술 발전과 함께 전통 문화와 역사를 소중히 여기는 한국은 매력적인 나라로 손꼽힌다.\n",
      "CPU times: total: 125 ms\n",
      "Wall time: 3.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time # 실행시간 측정\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# 인메모리 캐시를 사용합니다.\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# 체인을 실행합니다.\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 체인을 실행합니다.\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLiteCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.cache import SQLiteCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "\n",
    "# 캐시 디렉토리를 생성합니다.\n",
    "if not os.path.exists(\"cache\"):\n",
    "    os.makedirs(\"cache\")\n",
    "\n",
    "# SQLiteCache를 사용합니다.\n",
    "set_llm_cache(SQLiteCache(database_path=\"cache/llm_cache.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 체인을 실행합니다.\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메모리 (Memory)\n",
    "\n",
    "- 대화 기록을 저장하는 방법 (history)\n",
    "\n",
    "### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain 추가하는 방법\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# ChatOpenAI 모델을 초기화합니다.\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 대화형 프롬프트를 생성합니다. 이 프롬프트는 시스템 메시지, 이전 대화 내역, 그리고 사용자 입력을 포함합니다.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 버퍼 메모리를 생성하고, 메시지 반환 기능을 활성화합니다.\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})  # 메모리 변수를 빈 딕셔너리로 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables)\n",
    "    | itemgetter(\"chat_history\")  # memory_key 와 동일하게 입력합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable.invoke({\"input\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = runnable | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 객체의 invoke 메서드를 사용하여 입력에 대한 응답을 생성합니다.\n",
    "response = chain.invoke({\"input\": \"만나서 반갑습니다. 제 이름은 제이슨입니다.\"})\n",
    "print(response.content)  # 생성된 응답을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"human\": \"만나서 반갑습니다. 제 이름은 제이슨입니다.\"}, {\"ai\": response.content}\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이름을 기억하고 있는지 추가 질의합니다.\n",
    "response = chain.invoke({\"input\": \"제 이름이 무엇이었는지 기억하세요?\"})\n",
    "# 답변을 출력합니다.\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 대화 내용을 자동으로 저장하는 커스텀 메모리 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, Runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConversationChain(Runnable):\n",
    "\n",
    "    def __init__(self, llm, prompt, memory, input_key=\"input\"):\n",
    "\n",
    "        self.prompt = prompt\n",
    "        self.memory = memory\n",
    "        self.input_key = input_key\n",
    "\n",
    "        self.chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                chat_history=RunnableLambda(self.memory.load_memory_variables)\n",
    "                | itemgetter(memory.memory_key)  # memory_key 와 동일하게 입력합니다.\n",
    "            )\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def invoke(self, query, configs=None, **kwargs):\n",
    "        answer = self.chain.invoke({self.input_key: query})\n",
    "        self.memory.save_context(inputs={\"human\": query}, outputs={\"ai\": answer})\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 메모리 종류\n",
    "\n",
    "- ConversationBufferMemory(대화 버퍼 메모리): 메시지를 저장한 다음 변수에 메시지를 추출\n",
    "- ConversationBufferWindowMemory(대화 버퍼 윈도우 메모리): 시간이 지남에 따라 k개만큼만 사용\n",
    "- ConversationTokenBufferMemory(대화 토큰 버퍼 메모리): 최근 대화 저장 시 토큰 길이를 사용\n",
    "- ConversationEntityMemory(대화 엔티티 메모리): 특정 엔티티에 대한 주어진 사실을 기억\n",
    "- ConversationKGMemory(대화 지식그래프 메모리): 지식 그래프를 활용해 서로 다른 개체 간 관계를 이해\n",
    "- ConversationSummaryMemory(대화 요약 메모리): 대화의 요약을 생성하여 기억\n",
    "- VectorStoreRetrieverMemory(벡터저장소 검색 메모리): 벡터 스토어에서 가장 눈에 띄는 상위 K개 문서 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOpenAI 모델을 초기화합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 대화형 프롬프트를 생성합니다. 이 프롬프트는 시스템 메시지, 이전 대화 내역, 그리고 사용자 입력을 포함합니다.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 대화 버퍼 메모리를 생성하고, 메시지 반환 기능을 활성화합니다.\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "\n",
    "# 요약 메모리로 교체할 경우\n",
    "# memory = ConversationSummaryMemory(\n",
    "#     llm=llm, return_messages=True, memory_key=\"chat_history\"\n",
    "# )\n",
    "\n",
    "conversation_chain = MyConversationChain(llm, prompt, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain.invoke(\"안녕하세요? 만나서 반갑습니다. 제 이름은 제이슨 입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain.invoke(\"제 이름이 뭐라고요?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLite에 대화내용 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "# SQLChatMessageHistory 객체를 생성하고 세션 ID와 데이터베이스 연결 파일을 설정\n",
    "chat_message_history = SQLChatMessageHistory(\n",
    "    session_id=\"sql_history\", connection=\"sqlite:///sqlite.db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 메시지를 추가합니다.\n",
    "chat_message_history.add_user_message(\n",
    "    \"안녕? 만나서 반가워. 내 이름은 제이슨이야. 나는 랭체인 개발자야. 앞으로 잘 부탁해!\"\n",
    ")\n",
    "# AI 메시지를 추가합니다.\n",
    "chat_message_history.add_ai_message(\"안녕 제이슨, 만나서 반가워. 나도 잘 부탁해!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 메시지 기록의 메시지들\n",
    "chat_message_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # 시스템 메시지\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        # 대화 기록을 위한 Placeholder\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),  # 질문\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain 을 생성합니다.\n",
    "chain = prompt | ChatOpenAI(model_name=\"gpt-4o\") | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_history(user_id, conversation_id):\n",
    "    return SQLChatMessageHistory(\n",
    "        table_name=user_id,\n",
    "        session_id=conversation_id,\n",
    "        connection=\"sqlite:///sqlite.db\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.utils import ConfigurableFieldSpec\n",
    "\n",
    "config_fields = [\n",
    "    ConfigurableFieldSpec(\n",
    "        id=\"user_id\",\n",
    "        annotation=str,\n",
    "        name=\"User ID\",\n",
    "        description=\"Unique identifier for a user.\",\n",
    "        default=\"\",\n",
    "        is_shared=True,\n",
    "    ),\n",
    "    ConfigurableFieldSpec(\n",
    "        id=\"conversation_id\",\n",
    "        annotation=str,\n",
    "        name=\"Conversation ID\",\n",
    "        description=\"Unique identifier for a conversation.\",\n",
    "        default=\"\",\n",
    "        is_shared=True,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_chat_history,  # 대화 기록을 가져오는 함수를 설정합니다.\n",
    "    input_messages_key=\"question\",  # 입력 메시지의 키를 \"question\"으로 설정\n",
    "    history_messages_key=\"chat_history\",  # 대화 기록 메시지의 키를 \"history\"로 설정\n",
    "    history_factory_config=config_fields,  # 대화 기록 조회시 참고할 파라미터를 설정합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 설정\n",
    "config = {\"configurable\": {\"user_id\": \"user1\", \"conversation_id\": \"conversation1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 config 를 전달하여 실행합니다.\n",
    "chain_with_history.invoke({\"question\": \"안녕 반가워, 내 이름은 제이슨이야\"}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후속 질문을 실해합니다.\n",
    "chain_with_history.invoke({\"question\": \"내 이름이 뭐라고?\"}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 설정\n",
    "config = {\"configurable\": {\"user_id\": \"user1\", \"conversation_id\": \"conversation2\"}}\n",
    "\n",
    "# 질문과 config 를 전달하여 실행합니다.\n",
    "chain_with_history.invoke({\"question\": \"내 이름이 뭐라고?\"}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
